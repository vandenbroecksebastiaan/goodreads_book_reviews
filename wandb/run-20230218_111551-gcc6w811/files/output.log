Epoch:   0%|                                                                                                                     | 0/1 [00:00<?, ?it/s]
2it [00:01,  1.29it/s]
















Traceback (most recent call last):
  File "/home/sebastiaan/fun/goodreads_book_reviews/main.py", line 26, in <module>
    main()
  File "/home/sebastiaan/fun/goodreads_book_reviews/main.py", line 22, in main
    train(model, train_loader, eval_loader)
  File "/home/sebastiaan/fun/goodreads_book_reviews/train.py", line 37, in train
    eval_metrics = validation_step(model, criterion, eval_loader)
  File "/home/sebastiaan/fun/goodreads_book_reviews/train.py", line 48, in validation_step
    idx, (input_id, attention_mask, eval_target) = next(enumerate(eval_loader))
  File "/home/sebastiaan/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 628, in __next__
    data = self._next_data()
  File "/home/sebastiaan/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 671, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/sebastiaan/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 58, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/sebastiaan/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 58, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/sebastiaan/.local/lib/python3.10/site-packages/torch/utils/data/dataset.py", line 295, in __getitem__
    return self.dataset[self.indices[idx]]
  File "/home/sebastiaan/fun/goodreads_book_reviews/data.py", line 65, in __getitem__
    x_data = self.tokenizer(self.review_text[index],
  File "/home/sebastiaan/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2523, in __call__
    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)
  File "/home/sebastiaan/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2629, in _call_one
    return self.encode_plus(
  File "/home/sebastiaan/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2702, in encode_plus
    return self._encode_plus(
  File "/home/sebastiaan/.local/lib/python3.10/site-packages/transformers/tokenization_utils.py", line 649, in _encode_plus
    first_ids = get_input_ids(text)
  File "/home/sebastiaan/.local/lib/python3.10/site-packages/transformers/tokenization_utils.py", line 616, in get_input_ids
    tokens = self.tokenize(text, **kwargs)
  File "/home/sebastiaan/.local/lib/python3.10/site-packages/transformers/tokenization_utils.py", line 547, in tokenize
    tokenized_text.extend(self._tokenize(token))
  File "/home/sebastiaan/.local/lib/python3.10/site-packages/transformers/models/distilbert/tokenization_distilbert.py", line 201, in _tokenize
    for token in self.basic_tokenizer.tokenize(text, never_split=self.all_special_tokens):
  File "/home/sebastiaan/.local/lib/python3.10/site-packages/transformers/models/distilbert/tokenization_distilbert.py", line 376, in tokenize
    text = self._clean_text(text)
  File "/home/sebastiaan/.local/lib/python3.10/site-packages/transformers/models/distilbert/tokenization_distilbert.py", line 478, in _clean_text
    if _is_whitespace(char):
  File "/home/sebastiaan/.local/lib/python3.10/site-packages/transformers/tokenization_utils.py", line 272, in _is_whitespace
    if char == " " or char == "\t" or char == "\n" or char == "\r":
KeyboardInterrupt
Traceback (most recent call last):
  File "/home/sebastiaan/fun/goodreads_book_reviews/main.py", line 26, in <module>
    main()
  File "/home/sebastiaan/fun/goodreads_book_reviews/main.py", line 22, in main
    train(model, train_loader, eval_loader)
  File "/home/sebastiaan/fun/goodreads_book_reviews/train.py", line 37, in train
    eval_metrics = validation_step(model, criterion, eval_loader)
  File "/home/sebastiaan/fun/goodreads_book_reviews/train.py", line 48, in validation_step
    idx, (input_id, attention_mask, eval_target) = next(enumerate(eval_loader))
  File "/home/sebastiaan/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 628, in __next__
    data = self._next_data()
  File "/home/sebastiaan/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 671, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/sebastiaan/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 58, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/sebastiaan/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 58, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/sebastiaan/.local/lib/python3.10/site-packages/torch/utils/data/dataset.py", line 295, in __getitem__
    return self.dataset[self.indices[idx]]
  File "/home/sebastiaan/fun/goodreads_book_reviews/data.py", line 65, in __getitem__
    x_data = self.tokenizer(self.review_text[index],
  File "/home/sebastiaan/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2523, in __call__
    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)
  File "/home/sebastiaan/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2629, in _call_one
    return self.encode_plus(
  File "/home/sebastiaan/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2702, in encode_plus
    return self._encode_plus(
  File "/home/sebastiaan/.local/lib/python3.10/site-packages/transformers/tokenization_utils.py", line 649, in _encode_plus
    first_ids = get_input_ids(text)
  File "/home/sebastiaan/.local/lib/python3.10/site-packages/transformers/tokenization_utils.py", line 616, in get_input_ids
    tokens = self.tokenize(text, **kwargs)
  File "/home/sebastiaan/.local/lib/python3.10/site-packages/transformers/tokenization_utils.py", line 547, in tokenize
    tokenized_text.extend(self._tokenize(token))
  File "/home/sebastiaan/.local/lib/python3.10/site-packages/transformers/models/distilbert/tokenization_distilbert.py", line 201, in _tokenize
    for token in self.basic_tokenizer.tokenize(text, never_split=self.all_special_tokens):
  File "/home/sebastiaan/.local/lib/python3.10/site-packages/transformers/models/distilbert/tokenization_distilbert.py", line 376, in tokenize
    text = self._clean_text(text)
  File "/home/sebastiaan/.local/lib/python3.10/site-packages/transformers/models/distilbert/tokenization_distilbert.py", line 478, in _clean_text
    if _is_whitespace(char):
  File "/home/sebastiaan/.local/lib/python3.10/site-packages/transformers/tokenization_utils.py", line 272, in _is_whitespace
    if char == " " or char == "\t" or char == "\n" or char == "\r":

0 26 tensor(1.9127, device='cuda:0', grad_fn=<NllLossBackward0>) tensor([0, 3, 2, 4, 2, 2, 0, 2, 0, 0], device='cuda:0')