
Epoch:   0%|                                                                                                                   | 0/1 [00:00<?, ?it/s]
 0 0 1.9348963499069214
 0 1 2.01861834526062
 0 2 1.959362506866455
 0 3 1.9819170236587524
 0 4 1.9573231935501099
 0 5 1.9627013206481934
 0 6 1.9341355562210083
 0 7 1.980591893196106
 0 8 1.9850645065307617
 0 9 1.9180793762207031
 0 10 1.9778876304626465
 0 11 1.9081918001174927
 0 12 1.9512017965316772
 0 13 1.9974706172943115
 0 14 1.8943427801132202
 0 15 1.894395351409912
 0 16 1.938763976097107
 0 17 1.9052194356918335
 0 18 1.863181471824646
 0 19 1.8249473571777344
 0 20 1.9250612258911133
 0 21 1.932649850845337
 0 22 1.9446874856948853
 0 23 1.986585021018982
 0 24 1.923067569732666
 0 25 1.9706227779388428
 0 26 1.9174859523773193
 0 27 1.8826833963394165
 0 28 1.928907871246338
 0 29 1.9771504402160645
 0 30 1.8781602382659912
 0 31 1.8915486335754395
 0 32 1.8208606243133545
 0 33 1.8855589628219604
 0 34 1.8623735904693604
 0 35 1.9200783967971802
 0 36 1.926642894744873
 0 37 1.9360785484313965
 0 38 1.9516860246658325
 0 39 1.9278658628463745
 0 40 1.8890691995620728
 0 41 1.9279141426086426
 0 42 1.8819133043289185
 0 43 1.97056245803833
 0 44 1.93474280834198
 0 45 1.8269729614257812
 0 46 1.872812032699585
 0 47 1.85896897315979
 0 48 1.9272338151931763
 0 49 1.9267746210098267
 0 50 1.9105961322784424
 0 51 1.8888825178146362
 0 52 1.8588793277740479
 0 53 1.8647891283035278
 0 54 1.8312205076217651
 0 55 1.9067058563232422
 0 56 1.9846751689910889
 0 57 1.937436819076538
 0 58 1.8857148885726929
 0 59 1.9394160509109497
 0 60 1.9165985584259033
 0 61 1.8938876390457153
 0 62 1.9371984004974365
 0 63 1.809924602508545
 0 64 1.8179961442947388
 0 65 1.9521735906600952
 0 66 1.8555939197540283
 0 67 1.8955572843551636
 0 68 1.8913447856903076
 0 69 1.9198521375656128
 0 70 1.8941587209701538
 0 71 1.8694236278533936
 0 72 1.9835971593856812
 0 73 1.8542624711990356
 0 74 1.8532867431640625
 0 75 1.948288917541504
 0 76 1.8739356994628906
 0 77 1.9595974683761597
 0 78 1.9239821434020996
 0 79 1.935286521911621
 0 80 1.8553823232650757
 0 81 1.949972152709961
 0 82 1.8568410873413086
 0 83 1.8687999248504639
 0 84 1.7818448543548584
 0 85 1.8400484323501587
 0 86 1.8897844552993774
 0 87 1.9272571802139282
 0 88 1.984691858291626
 0 89 1.862605094909668
 0 90 1.8214688301086426
 0 91 1.877985954284668
 0 92 2.0005335807800293
 0 93 1.8236713409423828
 0 94 1.877974033355713
 0 95 1.8564233779907227
 0 96 1.9266060590744019
 0 97 1.7828454971313477
 0 98 1.8633787631988525
 0 100 1.882462978363037
 0 100 1.882462978363037
 0 101 2.0369999408721924
 0 102 1.957029104232788
 0 103 1.8973361253738403
 0 104 1.8297374248504639
 0 105 1.8485971689224243
 0 106 1.9399018287658691
 0 107 1.885722279548645
 0 108 1.7972937822341923
 0 109 1.9153693914413452
 0 111 1.9060202836990356
 0 114 1.8831335306167603
 0 114 1.8831335306167603
 0 117 1.9103740453720093
 0 120 1.9107050895690918
 0 123 1.8572461605072021
 0 126 1.8740719556808472
 0 129 1.9258853197097778
 0 129 1.9258853197097778
 0 132 1.8809168338775635
 0 135 1.8337973356246948
 0 138 1.9772038459777832
 0 141 1.8226544857025146
 0 141 1.8226544857025146
 0 144 1.8921684026718146
 0 147 1.9062640666961676
 0 150 1.8394397497177124
 0 153 1.7695491313934326
 0 153 1.7695491313934326
 0 156 1.8738822937011719
 0 159 1.9067530632019043
 0 162 1.8792934417724613
 0 165 1.9484885931015015
 0 168 1.8836048841476445
 0 168 1.8836048841476445
 0 171 2.0068826675415045
 0 174 1.8536137342453003
 0 177 1.8659337759017944
 0 180 1.9190276861190796
 0 180 1.9190276861190796
 0 183 1.8065435886383057
 0 186 1.8126206398010254
 0 189 1.8261772394180298
 0 189 1.8261772394180298
 0 192 1.8610708713531494
 0 195 1.8720600605010986
 0 198 1.8998320102691656
 0 201 1.7152011394500732
 0 201 1.7152011394500732
 0 204 1.7856763601303132
 0 207 1.8265049457550049
 0 210 1.7521761655807495
 0 213 1.8137650489807135
 0 216 1.8753159046173096
 0 216 1.8753159046173096
 0 219 1.8438673019409186
 0 222 1.8367936611175537
 0 225 1.9011501073837287
 0 228 1.8941130638122559
 0 228 1.8941130638122559
 0 231 1.8133965730667114
 0 234 1.8631557226181034
 0 237 1.8442285060882568
 0 240 1.8363296985626228
 0 243 1.8170218467712402
 0 243 1.8170218467712402
 0 246 1.8673784732818604
 0 249 1.8467748165130615
 0 252 1.7496764659881592
 0 255 1.8128684759140015
 0 255 1.8128684759140015
 0 258 1.8079457283020025
 0 261 1.8499711751937866
 0 264 1.8684259653091436
 0 267 1.7954982519149786
 0 270 1.8705613613128662
 0 270 1.8705613613128662
 0 273 1.8069742918014526
 0 276 1.8355596065521246
 0 279 1.8627712726593018
 0 279 1.8627712726593018
 0 282 1.8262989521026611
 0 285 1.8145921230316162
 0 288 1.7380701303482056
 0 291 1.8476889133453376
 0 291 1.8476889133453376
 0 294 1.8191158771514893
 0 297 1.8341083526611328
 0 300 1.9034798145294198
 0 303 1.7012790441513062
 0 306 1.9030598402023315
 0 306 1.9030598402023315
 0 309 1.8020696640014648
 0 312 1.7822856903076172
 0 315 1.7886375188827515
 0 318 1.8000905513763428
 0 318 1.8000905513763428
 0 321 1.9271922111511238
 0 324 1.8171821832656868
 0 327 1.8475389480590828
 0 330 1.8864926099777222
 0 330 1.8864926099777222
 0 333 1.9200644493103027
 0 336 1.7399873733520508
 0 339 1.7520469427108765
 0 342 1.8384304046630865
 0 342 1.8384304046630865
 0 345 1.8658492565155035
 0 348 1.7144982814788818
 0 351 1.8349274396896362
 0 354 1.8403919935226442
 0 354 1.8403919935226442
 0 357 1.8178334236145022
 0 360 1.8092949390411377
 0 363 1.7303235530853271
 0 366 1.8258221149444581
 0 366 1.8258221149444581
 0 369 1.7988379001617432
 0 372 1.8055453300476074
 0 375 1.9138687849044874
 0 378 1.7644543647766113
 0 378 1.7644543647766113
 0 381 1.8399182558059692
 0 384 1.8060057163238525
 0 387 1.7770649194717407
 0 390 1.7800061702728271
 0 390 1.7800061702728271
 0 393 1.8328402042388916
 0 396 1.7560501098632812
 0 399 1.7560142278671265
 0 402 1.8146458864212036
 0 405 1.7582738399505615
 0 405 1.7582738399505615
 0 408 1.7452440261840825
 0 411 1.7139793634414673
 0 414 1.7080671787261963
 0 417 1.7063901424407963
 0 417 1.7063901424407963
 0 420 1.7787026166915894
 0 423 1.7974560260772705
 0 426 1.8319715261459355
 0 429 1.8225027322769165
 0 429 1.8225027322769165
 0 432 1.7975018024444585
 0 435 1.7806501388549805
 0 438 1.7007566690444946
 0 441 1.6563640832901946
 0 441 1.6563640832901946
 0 444 1.8104875087738037
 0 447 1.7464203834533691
 0 450 1.7259968519210815
 0 453 1.6972103118896484
 0 456 1.6923469305038452
 0 456 1.6923469305038452
 0 459 1.7094105482101442
 0 462 1.6577839851379395
 0 465 1.8067487478256226
 0 468 1.7318389415740967
 0 468 1.7318389415740967
 0 471 1.7947537899017334
 0 474 1.7244366407394414
 0 477 1.8158918619155884
 0 480 1.7455737590789795
 0 480 1.7455737590789795
 0 483 1.7947913408279425
 0 486 1.7558209896087646
 0 489 1.7147225141525269
 0 492 1.8309141397476196
 0 495 1.8336251974105835
 0 495 1.8336251974105835
 0 498 1.8060513734817505
 0 501 1.6501848697662354
 0 504 1.7505464553833008
 0 507 1.8543920516967773
 0 507 1.8543920516967773
 0 510 1.7647596597671509
 0 513 1.6815050840377808
 0 516 1.8020094633102417
 0 519 1.7849568128585815
 0 519 1.7849568128585815
 0 524 1.7493143081665045
 0 524 1.7493143081665045
 0 529 1.6996864080429077
 0 529 1.6996864080429077
 0 529 1.6996864080429077
 0 536 1.7239989042282104
 0 536 1.7239989042282104
 0 541 1.7513399124145508
 0 541 1.7513399124145508
 0 546 1.6695966720581055
 0 546 1.6695966720581055
 0 546 1.6695966720581055
 0 553 1.6940602064132695
 0 553 1.6940602064132695
 0 558 1.6415410041809082
 0 558 1.6415410041809082
Epoch:   0%|                                                                                                                   | 0/1 [07:43<?, ?it/s]
Traceback (most recent call last):
  File "/home/sebastiaan/fun/goodreads_book_reviews/main.py", line 27, in <module>
    main()
  File "/home/sebastiaan/fun/goodreads_book_reviews/main.py", line 23, in main
    train(model, train_loader, eval_loader)
  File "/home/sebastiaan/fun/goodreads_book_reviews/train.py", line 35, in train
    train_loss.backward()
  File "/home/sebastiaan/.local/lib/python3.10/site-packages/torch/_tensor.py", line 488, in backward
    torch.autograd.backward(
  File "/home/sebastiaan/.local/lib/python3.10/site-packages/torch/autograd/__init__.py", line 197, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/home/sebastiaan/.local/lib/python3.10/site-packages/wandb/wandb_torch.py", line 282, in <lambda>
    handle = var.register_hook(lambda grad: _callback(grad, log_track))
  File "/home/sebastiaan/.local/lib/python3.10/site-packages/wandb/wandb_torch.py", line 280, in _callback
    self.log_tensor_stats(grad.data, name)
  File "/home/sebastiaan/.local/lib/python3.10/site-packages/wandb/wandb_torch.py", line 215, in log_tensor_stats
    if self._no_finite_values(flat):
  File "/home/sebastiaan/.local/lib/python3.10/site-packages/wandb/wandb_torch.py", line 303, in _no_finite_values
    return tensor.shape == torch.Size([0]) or (~torch.isfinite(tensor)).all().item()
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 150.00 MiB (GPU 0; 11.75 GiB total capacity; 11.17 GiB already allocated; 21.50 MiB free; 11.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "/home/sebastiaan/fun/goodreads_book_reviews/main.py", line 27, in <module>
    main()
  File "/home/sebastiaan/fun/goodreads_book_reviews/main.py", line 23, in main
    train(model, train_loader, eval_loader)
  File "/home/sebastiaan/fun/goodreads_book_reviews/train.py", line 35, in train
    train_loss.backward()
  File "/home/sebastiaan/.local/lib/python3.10/site-packages/torch/_tensor.py", line 488, in backward
    torch.autograd.backward(
  File "/home/sebastiaan/.local/lib/python3.10/site-packages/torch/autograd/__init__.py", line 197, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/home/sebastiaan/.local/lib/python3.10/site-packages/wandb/wandb_torch.py", line 282, in <lambda>
    handle = var.register_hook(lambda grad: _callback(grad, log_track))
  File "/home/sebastiaan/.local/lib/python3.10/site-packages/wandb/wandb_torch.py", line 280, in _callback
    self.log_tensor_stats(grad.data, name)
  File "/home/sebastiaan/.local/lib/python3.10/site-packages/wandb/wandb_torch.py", line 215, in log_tensor_stats
    if self._no_finite_values(flat):
  File "/home/sebastiaan/.local/lib/python3.10/site-packages/wandb/wandb_torch.py", line 303, in _no_finite_values
    return tensor.shape == torch.Size([0]) or (~torch.isfinite(tensor)).all().item()
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 150.00 MiB (GPU 0; 11.75 GiB total capacity; 11.17 GiB already allocated; 21.50 MiB free; 11.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF