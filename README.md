This is my work for the [kaggle competition](https://www.kaggle.com/competitions/goodreads-books-reviews-290312/overview).
In this project, we leverage the power of DistilBERT to build a text mining model for book reviews that predicts the rating of a book based on its textual description. Our model takes advantage of the pre-trained DistilBERT architecture, which allows us to transfer the knowledge learned from a large corpus of text to the specific task of book review rating prediction. By using this model, we aim to provide an efficient and accurate solution to the problem of book rating prediction, which has important implications for both readers and publishers. Overall, this model showcases the potential of deep learning models for natural language processing tasks, and highlights the importance of pre-trained models like DistilBERT in advancing the state of the art in this field.